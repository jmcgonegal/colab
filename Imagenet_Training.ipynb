{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Imagenet Training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmcgonegal/colab/blob/master/Imagenet_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "aQKegMPTGM2A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import applications\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Flatten, Dense, Convolution2D, MaxPooling2D, Activation, BatchNormalization, GlobalAveragePooling2D\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
        "\n",
        "# path to the model weights files.\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 384, 384\n",
        "\n",
        "train_data_dir = 'train2' # directory with images and \n",
        "validation_data_dir = 'validation'\n",
        "epochs = 2000\n",
        "batch_size = 3 # how many images to train in the batch (limited by size of memory)\n",
        "steps_per_epoch=200 # training steps\n",
        "validation_steps=75 # validation steps\n",
        "class_mode='categorical'\n",
        "\n",
        "# prepare data augmentation configuration\n",
        "train_datagen = ImageDataGenerator(\n",
        "    #rotation_range=5,\n",
        "    #width_shift_range=0.2,\n",
        "    #height_shift_range=0.2,\n",
        "    rescale=1. / 255,\n",
        "    #shear_range=0.1,\n",
        "    #zoom_range=0.2,\n",
        "    #horizontal_flip=True, \n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "# images we will use for training\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    color_mode='rgb',\n",
        "    class_mode=class_mode,\n",
        "    subset='training',\n",
        "    shuffle=True)\n",
        "\n",
        "# images we will use for validation\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir, # validation_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    color_mode='rgb',\n",
        "    subset='validation',\n",
        "    class_mode=class_mode)\n",
        "\n",
        "# classes found for training\n",
        "class_dictionary = train_generator.class_indices\n",
        "num_classes = len(class_dictionary)\n",
        "\n",
        "# our image shape width,height,rgb\n",
        "input_shape = (img_width, img_height, 3)\n",
        "\n",
        "# build the VGG16/VGG19 network\n",
        "vgg_model = applications.VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "# dont adjust weights of these layers\n",
        "for i in range(len(vgg_model.layers)):\n",
        "    vgg_model.layers[i].trainable = False\n",
        "    \n",
        "# build our training model\n",
        "top_model = Sequential()\n",
        "top_model.add(vgg_model)\n",
        "top_model.add(Flatten(input_shape=input_shape))\n",
        "top_model.add(Dense(256, activation='relu'))\n",
        "top_model.add(Dropout(0.5))\n",
        "top_model.add(Dense(num_classes, activation='sigmoid'))\n",
        "\n",
        "top_model.summary()\n",
        "\n",
        "#filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "filepath=\"weights.best.hdf5\"\n",
        "\n",
        "# only do this if file exists?\n",
        "#top_model.load_weights(filepath)\n",
        "\n",
        "# compile the model with a SGD/momentum optimizer\n",
        "# and a very slow learning rate.\n",
        "top_model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=optimizers.Adam(lr=1e-5), \n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "print('Model loaded')\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=batch_size, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\n",
        "early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
        "\n",
        "callbacks_list = [checkpoint, tensorboard] # early_stop,"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "clBuFSMTGigT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# fit the model\n",
        "history = top_model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=callbacks_list,\n",
        "    max_queue_size=50,\n",
        "    workers=4,\n",
        "    use_multiprocessing=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}